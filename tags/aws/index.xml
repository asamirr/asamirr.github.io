<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>aws on Ahmed Samir</title>
    <link>https://asamirr.github.io/tags/aws/</link>
    <description>Ahmed Samir (aws)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Apr 2020 22:40:11 +0200</lastBuildDate>
    
    <atom:link href="https://asamirr.github.io/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Notes on AWS Fundamentals - Addressing Security Risks Week 3</title>
      <link>https://asamirr.github.io/post/aws-security-week3-notes/</link>
      <pubDate>Tue, 14 Apr 2020 22:40:11 +0200</pubDate>
      
      <guid>https://asamirr.github.io/post/aws-security-week3-notes/</guid>
      <description>&lt;p&gt;In week 3, we delve right into encryption and data protection. We start by revisiting the concept of &amp;ldquo;least privileged access&amp;rdquo; and how it is important to provide minimum permissions for users of your resources. These users could be infra users or even other AWS services. For example, a lambda function that pulls out data from an S3 bucket should have Read Only access to prevent data exposure to the unauthorized.&lt;/p&gt;
&lt;p&gt;Then we go into data types and how often do we access it. For each data type you have, there&amp;rsquo;s a complimentary AWS service for it to be stored effectively. So let&amp;rsquo;s you have objects to store or even log files, you could store them in an &lt;strong&gt;AWS S3&lt;/strong&gt; bucket. If you don&amp;rsquo;t need this data in a hurry, you could just dump it in &lt;strong&gt;Amazon Glacier&lt;/strong&gt; to lower your costs even more. A good practice to follow is to set up lifecycle policies for your data in S3 to be backed up after a certain time and moved to Glacier.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll also to store data for your application. For that you might need &lt;strong&gt;Amazon RDS&lt;/strong&gt; which is a relational database solution that comes with several server options like MySQL and Postgres. Other applications don&amp;rsquo;t really follow the relational model for storing data so you might want to check out &lt;strong&gt;Amazon DynamoDB&lt;/strong&gt; for a NoSQL database which is so much faster than a relational database. You could attach &lt;strong&gt;DynamoDB Accelerator&lt;/strong&gt; to your DynamoDB tables, which is an in-memory cache that lower the response time to microseconds.&lt;/p&gt;
&lt;p&gt;For data generated from our EC2 instances, there are &lt;strong&gt;AWS Elastic Block Store&lt;/strong&gt; and &lt;strong&gt;Elastic File System&lt;/strong&gt;. For EC2 instances, if you shut them down you&amp;rsquo;ll lose everything that was on it. But if you attach an EBS volume to it, you could always retain the data that was on that instance. EFS is helpful if you need a file system for your instance and it can automatically grow and shrink depending on the amount of files you have.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s &lt;strong&gt;AWS Redshift&lt;/strong&gt; for secure data warehousing. Mix it with the ability of parallel query execution and you&amp;rsquo;ll see wonders! You could also access data in S3 via &lt;strong&gt;Redshift Spectrum&lt;/strong&gt; allowing you to query on more data and get up-to-date results. If you&amp;rsquo;re too lazy for Redshift, you could go for &lt;strong&gt;Amazon Athena&lt;/strong&gt; which allows you to run SQL queries on data stored in S3.&lt;/p&gt;
&lt;h2 id=&#34;encrypting-data-in-transit&#34;&gt;&lt;strong&gt;Encrypting Data in Transit&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;First, you have to categorize your data based on levels of sensitivity. Then, you apply encryption standards for each category. You also need to authenticate network communications including verifying the identity of communications by using a protocol like &lt;strong&gt;TLS&lt;/strong&gt; or &lt;strong&gt;IPSec&lt;/strong&gt; (IPSec is a protocol for intrinsic data protection between host). IPSec is a protocol to protect communication at the network layer which is layer three of the OS I model. TLS is used for encrypting information at the transport layer which is layer four of the OSI model. TLS is the successor protocol to SSL. TLS is an improved version of SSL. It works in much the same way as the SSL, using encryption to protect the transfer of data and information.&lt;/p&gt;
&lt;p&gt;You should use HTTPS instead of HTTP for data transmission to prevent eavesdropping, unauthorized alterations, and unauthorized copying of your data. SSL and TLS use X.509 certificate to authenticate the server. You want to make sure you store X.509 certificates securely and rotate them with strict access control. That could be handled by &lt;strong&gt;AWS Certificate Manager&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;encrypting-data-in-rest&#34;&gt;&lt;strong&gt;Encrypting Data in Rest&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;The concept of &lt;em&gt;Encryption at Rest&lt;/em&gt; could be demonstrated by imagining you have a locked safe in your house, you have the key to unlock it anytime and you have it on you wherever you go. AWS has you covered in that case. There are services that give you lets you create and manage your keys or you could choose to have AWS manage it for you and generate new keys regularly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Amazon Key Management Service&lt;/strong&gt; is our first one here and it helps you create and control *customer master keys *, the encryption keys used to encrypt your data. When creating it, you define key usage permissions. That means you pick users and roles who are allowed to use it and perform any encryption/decryption on your data.&lt;/p&gt;
&lt;h2 id=&#34;database-encryption&#34;&gt;&lt;strong&gt;Database Encryption&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Now we talk about how we encrypt our data and secure it. By default, you probably know that most of the times databases are stored in a private subnet. For you to send get there, you&amp;rsquo;ll have to send a request which will be received by a load balancer and then distributed to one of the web servers. After it&amp;rsquo;s validated, it&amp;rsquo;ll be sent to an internal load balancer inside the VPC and then distributed to the application servers. Then, the servers will talk with the database to get info about your request and see if it should return the data you&amp;rsquo;re requesting or not.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a visual demonstration for better understanding:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;static/image-20200414221138118.png&#34; alt=&#34;image-20200414221138118&#34;&gt;&lt;/p&gt;
&lt;p&gt;But it all comes back to your data classification and levels of sensitivity. So it makes sense that encrypting data at rest is crucial if you&amp;rsquo;re storing users&amp;rsquo; credit card info. You could use SSL to encrypt the connection to your &lt;strong&gt;RDS&lt;/strong&gt; instance for example. Enable the encryption option when you&amp;rsquo;re first creating the RDS instance, your instance gets armored with AES-256 encryption algorithm and your data is encrypted and ready to fight!&lt;/p&gt;
&lt;p&gt;Of course it must&amp;rsquo;ve struck your head by now that the keys you&amp;rsquo;ll use to encrypt/decrypt data on your RDS instance will be managed by &lt;strong&gt;AWS KMS&lt;/strong&gt;. That&amp;rsquo;s what it was built for yes!!&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re using a NoSQL database or DynamoDB, don&amp;rsquo;t worry because encryption is enabled by default to your DynamoDB tables and can&amp;rsquo;t be disabled.&lt;/p&gt;
&lt;h2 id=&#34;securing-of-s3&#34;&gt;&lt;strong&gt;Securing of S3&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;For starters, things like only allowing bucket access to authorized parties or disabling public access for the bucket should be considered. But it&amp;rsquo;s recommended that you enable default encryption on your S3 bucket encrypting all of your bucket&amp;rsquo;s objects.&lt;/p&gt;
&lt;p&gt;Another lazy option would be &lt;strong&gt;Amazon Macie&lt;/strong&gt;. Macie uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Automatically recognizing your sensitive data and providing you with alerts and giving you insights on how the data is managed and accessed.&lt;/p&gt;
&lt;h2 id=&#34;encrypting-data-on-ebs&#34;&gt;&lt;strong&gt;Encrypting Data on EBS&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;To encrypt data on an EBS volume, you configure your account to enable encryption on all your EBS volumes upon creation. One of EBS&amp;rsquo;s features is that it backups up data to S3 taking snapshots. Snapshots are a point-in-time copy of your EBS volume. If you have encryption enabled, these snapshots will be encrypted as well. EBS encryption enables data at rest security by encrypting your data volumes, boot volumes and snapshots using Amazon-managed keys or keys you create and manage using the AWS Key Management Service (KMS). In addition, the encryption occurs on the servers that host EC2 instances, providing encryption of data as it moves between EC2 instances and EBS data and boot volumes.&lt;/p&gt;
&lt;p&gt;Resources:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/&#34;&gt;https://docs.aws.amazon.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/aws-fundamentals-addressing-security-risk&#34;&gt;https://www.coursera.org/learn/aws-fundamentals-addressing-security-risk&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Notes on AWS Fundamentals - Addressing Security Risks Week 2</title>
      <link>https://asamirr.github.io/post/aws-security-week2-notes/</link>
      <pubDate>Mon, 13 Apr 2020 23:00:44 +0200</pubDate>
      
      <guid>https://asamirr.github.io/post/aws-security-week2-notes/</guid>
      <description>&lt;p&gt;Week 2 is all about monitoring and infrastructure security. We start by talking about network isolation and how AWS has a limited number of access points which they call &lt;strong&gt;endpoints&lt;/strong&gt; that help in monitoring the network traffic. Basically an &lt;strong&gt;endpoint&lt;/strong&gt; is a URL to gain access to a web service. Also, each service has a default endpoint in any region but you can use an alternate one for your API requests. That means if a service supports regions, the resources in each region are independent of similar resources in other regions.&lt;/p&gt;
&lt;p&gt;AWS offers &lt;strong&gt;Virtual Private Cloud&lt;/strong&gt; service to provide you with an isolated private network in the cloud. It uses IP address spaces that are allocated by you. You also have the ability to connect your on-premises environment with AWS VPC using services like &lt;strong&gt;IPSec tunnel&lt;/strong&gt; and &lt;strong&gt;AWS Direct Connect&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The instructor explained the concept behind VPC in a really cool way. He thought of it as building a fort and we need to build hives inside of it. The fort being VPC and the hives could be anything like an EC2 instance or an RDS cluster. So if I want that EC2 instance to be accessible, I&amp;rsquo;ll put in a &lt;strong&gt;public subnet&lt;/strong&gt;. A subnet is just a logical subdivision of our IPs. This is done via network access control lists or &lt;strong&gt;NACLs&lt;/strong&gt;. These lists contain IPs, which are allowed or denied access to the subnet. They are also stateless, so if you allow something in, you must explicitly allow it out via a rule. Now keep in mind that these rules are stateless. Meaning that you have to be specific with your outbound rule if you want your traffic to return.&lt;/p&gt;
&lt;p&gt;Regarding the RDS cluster, you don&amp;rsquo;t want that to be accessible through the internet. So you&amp;rsquo;ll have to put it in a private subnet so it won&amp;rsquo;t be visible to anyone. Now say you&amp;rsquo;re going the extra mile and want to make sure that EC2 instance is really safe. You could create a security group for it acting just like firewalls to control both inbound and outbound traffic to that instance.&lt;/p&gt;
&lt;p&gt;But things are way uglier out there in the real world and aren&amp;rsquo;t just safe. If we have more than a VPC and need them to communicate together, it&amp;rsquo;ll be tricky when you send your traffic out there unsecured. That&amp;rsquo;s AWS features like &lt;strong&gt;VPC Endpoints&lt;/strong&gt; or &lt;strong&gt;Private Links&lt;/strong&gt; come in handy! They&amp;rsquo;re simply establishing a private secure endpoint between one VPC and another so that they don&amp;rsquo;t leave your private network.&lt;/p&gt;
&lt;p&gt;Each VPC comes with a default main route table. &lt;strong&gt;Route Tables&lt;/strong&gt; are a set of routes that determine where the traffic is directed. Every subnet has to be associated with a route table, the main one or a custom one that you created.&lt;/p&gt;
&lt;h2 id=&#34;auditing-on-aws&#34;&gt;&lt;strong&gt;Auditing on AWS&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;First service used in auditing is &lt;strong&gt;AWS CloudTrail&lt;/strong&gt;. It enables you to track API calls, SDKs, CLI tools and other AWS services too. It lets you know who&amp;rsquo;s interacting with the infrastructure in your system and trace activity for the past 90 days via console.&lt;/p&gt;
&lt;p&gt;Next service is a really cool one and it&amp;rsquo;s &lt;strong&gt;AWS Config&lt;/strong&gt;. It&amp;rsquo;s pretty obvious from it&amp;rsquo;s name that it&amp;rsquo;s all about your configuration management. Config enables you to see every configuration made to any services your team is using across AWS and notifies you with any changes done. You could also tie it to CloudTrail and see who changed what and when. You could even have it execute a Lambda function to go and change it back to what it was before whoever changed it.&lt;/p&gt;
&lt;p&gt;Third one is &lt;strong&gt;Amazon Inspector&lt;/strong&gt; which runs an automated security assessment and check on anything not up to standards or vulnerabilities. At the end of the assessment, it generates its findings in the Inspector console with description of each finding and a recommendation on how to fix it.&lt;/p&gt;
&lt;p&gt;Last one is &lt;strong&gt;Trusted Advisor&lt;/strong&gt;. It provides you with real time guidance to help you provision your resources following AWS best practices. You run it and it generates a report about 5 categories cost optimization, performance, security, fault tolerance, and service limits. Each category will produce traffic light-based recommendations.&lt;/p&gt;
&lt;h2 id=&#34;monitoring-on-aws&#34;&gt;&lt;strong&gt;Monitoring on AWS&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;First service available is &lt;strong&gt;CloudWatch&lt;/strong&gt;. Simply it&amp;rsquo;s like having someone keep an eye on a service and provide metrics about so that later you could retrieve and review it. You could also fire up an alarm so when a certain metric exceeds a threshold you set up and performs some actions based on the value of the metric. For example, you can use CloudTrail to monitor activity related to changing an EC2 security group, and trigger an Amazon CloudWatch alarm when configuration changes happen. Also, &lt;strong&gt;CloudWatch Logs&lt;/strong&gt; is a helpful tool to store and access log files from different services. Like archiving log data from S3 in CloudWatch Logs or collecting and storing application logs.&lt;/p&gt;
&lt;p&gt;Another two cool tools that usually are used together &lt;strong&gt;Amazon GuardDuty&lt;/strong&gt; and &lt;strong&gt;Amazon Security Hub&lt;/strong&gt;. Let&amp;rsquo;s start with GuardDuty. A service that uses machine learning and anomaly detection to identify any malicious or unauthorized behavior in your account. It analyzes billions of events from your AWS resources like CloudTrail and VPC Flow Logs. You can also integrate GuardDuty with AWS CloudWatch Events or your current event management system to alert your security team. It provides three severity levels: low, medium, and high to help you prioritize responses to potential threats.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;re done with identifying and capturing these threats. Now it&amp;rsquo;s time to respond and luckily GuardDuty offers solutions like HTTPS APIs and &lt;strong&gt;Amazon CloudWatch Events&lt;/strong&gt; to support automated responses to these findings.&lt;/p&gt;
&lt;p&gt;Now you&amp;rsquo;re asking where can I find and look at all these security findings that were harvested from all of my accounts. With &lt;strong&gt;Amazon Security Hub&lt;/strong&gt; you have a single place where all of your security findings are organized and prioritized for you. They&amp;rsquo;re visually summarized in dashboards.&lt;/p&gt;
&lt;p&gt;Resources:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/&#34;&gt;https://docs.aws.amazon.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/aws-fundamentals-addressing-security-risk&#34;&gt;https://www.coursera.org/learn/aws-fundamentals-addressing-security-risk&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Notes on AWS Fundamentals - Addressing Security Risks Week 1</title>
      <link>https://asamirr.github.io/post/aws-security-week1-notes/</link>
      <pubDate>Mon, 13 Apr 2020 00:33:55 +0200</pubDate>
      
      <guid>https://asamirr.github.io/post/aws-security-week1-notes/</guid>
      <description>&lt;p&gt;We start with discussing the &amp;ldquo;Shared Responsibility&amp;rdquo; which simply defines security at AWS. It&amp;rsquo;s split to two parts. First part is all taken care of by AWS themselves or &lt;strong&gt;AWS responsibility “Security of the Cloud”&lt;/strong&gt;. They&amp;rsquo;re concerned by the physical security of the premises and facilities or simply the infrastructure and hardware in which all of the services you&amp;rsquo;re using operates. Second part is called &lt;strong&gt;Customer responsibility “Security in the Cloud”&lt;/strong&gt;. That goes from doing the configuration work of all the services you choose to work with, to protecting your users data, to regularly applying security patches and also making use of the security services provided by AWS to secure your applications that, for example, are deployed on an EC2 instance. AWS has tools like AWS Identity and Access Management (IAM), Amazon Cognito.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AWS Identity and Access Management (IAM)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s say you&amp;rsquo;re a tech lead at your company and you just created an AWS account. That&amp;rsquo;s when you take advantage of IAM! As the root user, you create an account for each engineer on the team and you only provision access to specific services that you know they&amp;rsquo;ll be using as a best practice. You have other controls like how the account could be accessed (via AWS Console access or Programmatic access) and then you attach policies needed for this type of account. Not only you could give restricted access to your users but also admin access could be provisioned by you through adding a JSON document that describes it.&lt;/p&gt;
&lt;p&gt;As an additional layer of security to accounts that have admin privileges, it&amp;rsquo;s considered a best practice to add MFA (stands for Multi-factor Authentication). It adds another verification check when you login and should be enabled for the root account and every other account with admin access. It can also be used to control to the service APIs. It&amp;rsquo;ll be a short code coming from an application like Google authenticator or a YubiKey.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Managing multiple accounts using AWS Organizations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One way to manage your account is to create IAM groups. We start by creating an IAM group for admin users, assign engineers to it that will need admin access to resources and from there we create more groups with each having suitable policies attached to it and assign users to them.&lt;/p&gt;
&lt;p&gt;But what happens when you start growing as an organization and multiple accounts have to be created? Surely, there&amp;rsquo;re ways and best practices in managing  multiple accounts on AWS. You want to make sure your admins could monitor the overall activity across the accounts and that each IAM group has only the permissions they need and nothing more. You want all the accounts&amp;rsquo; bills to be rolled up to one central account so that you pay for all the used services combined. Using &lt;strong&gt;AWS Organizations&lt;/strong&gt;, you can have split groups of accounts, monitor and govern access to AWS services by policies, automate account creation, have a single payment method for all accounts and share resources across all accounts. How it works?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You create a &lt;strong&gt;master&lt;/strong&gt; AWS account then create an organization in the master account. It&amp;rsquo;s recommended that you keep this account free of any operational AWS resources.&lt;/li&gt;
&lt;li&gt;Then, you could create &lt;strong&gt;Organizational Units&lt;/strong&gt;, which is simply a way to group some accounts together. Or you could create member accounts right away.&lt;/li&gt;
&lt;li&gt;Create &lt;strong&gt;service control policies&lt;/strong&gt; to be attached to OUs and restrict actions by member accounts. A good practice is to apply SCPs to OUs instead of individual accounts and make sure that every account belongs to an OU.&lt;/li&gt;
&lt;li&gt;Use &lt;strong&gt;AWS CloudTrail&lt;/strong&gt; in the master account to centrally track all AWS usage in the member accounts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Additional best practices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A great example was that you could define your application resources once on your Amazon VPC subnet and share them across your organization using the &lt;strong&gt;AWS Resource Access Manager&lt;/strong&gt; service (AWS RAM for short).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Authentication and Authorization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are several use cases but I&amp;rsquo;ll mention a couple. First one is if you have an internal reporting tool hosted on AWS and only employees can login and see the reports. But your company&amp;rsquo;s employees identities are on an Active Directory. You can simply federate (put them together?) this Active Directory with &lt;strong&gt;Amazon Cognito&lt;/strong&gt;. Another use case is to have resources talking to other resources and that&amp;rsquo;s where you need to validate the permissions of your app&amp;rsquo;s resources. Let&amp;rsquo;s say you have a containerized app running on EKS and needs to make an API call to query a table in an AWS DynamoDB. You&amp;rsquo;ll need to use &lt;strong&gt;IAM Roles&lt;/strong&gt;. These are short-term credentials that you&amp;rsquo;ll need to securely validate these API calls and manage permissions for actions that your app is allowed to perform on DynamoDB tables. &lt;strong&gt;IAM Roles&lt;/strong&gt; enable you to grant resources access to data without distributing passwords or API keys and by using them you&amp;rsquo;re applying the &amp;ldquo;granting least privilege access&amp;rdquo; concept.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Creating Policies in AWS&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We create IAM policies to users, groups or roles in order to assign permissions using the visual editor which is easier and doesn&amp;rsquo;t requiring to write JSON. You could always test your custom policy using the IAM policies simulator. IAM Policies types include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;AWS Managed Policy&lt;/strong&gt; which is created and administered by AWS and it&amp;rsquo;s designed for most common use cases.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Customer Managed Policy&lt;/strong&gt; which you could create and administer in your account. You could try to copy an existing Managed Policy and tailor it to your environment.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inline Policy&lt;/strong&gt; is a policy that&amp;rsquo;s embedded in an IAM identity. You can create a policy and embed it in a identity, either when you create the identity or later.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of the advantages to use managed policies over inline policies is &lt;strong&gt;Central change management&lt;/strong&gt;. When you change a managed policy, the change is applied to all principal entities that the policy is attached to. For example, if you want to add permission for a new AWS API, you can update the managed policy to add the permission. (If you&amp;rsquo;re using an AWS managed policy, AWS updates to the policy.) When the policy is updated, the changes are applied to all principal entities that the policy is attached to. In contrast, to change an inline policy you must individually edit each identity that contains the policy. For example, if a group and a role both contain the same inline policy, you must individually edit both principal entities in order to change that policy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Amazon Single Sign-On&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A service that lets you manage SSO access and user permissions across multiple AWS accounts in an AWS Organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Difference between Cognito User Pools and Identity Pools&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User Pools are for user registration and authentication. A Cognito User Pool handles all of this and as a developer you just need to use the SDK to retrieve user related information.&lt;/li&gt;
&lt;li&gt;An Identity Pool is a way to authorize your users to use the various AWS services. Like having access to an S3 bucket to upload a file. Identities could be coming from a Cognito User Pool or Facebook or Google.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Resources:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.aws.amazon.com/&#34;&gt;https://docs.aws.amazon.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/aws-fundamentals-addressing-security-risk&#34;&gt;https://www.coursera.org/learn/aws-fundamentals-addressing-security-risk&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>